<!DOCTYPE html>
<html>
  <head>
    <title>Allen Wang</title>
    <!-- Meta -->
    <meta charset="utf-8" />
    <meta
      name="description"
      content="Hi there, I'm Allen! I'm building intelligent systems to power a more sustainable world at Nectar."
    />
    <meta
      name="keywords"
      content="allen, wang, allenwang314, nectar, startup, AI, information geometry, mit, boston, cambridge, machine, learning, climate, tech, energy, acton, boxborough, math, engineer, design, development, hackathon, hacker, hackmit"
    />

    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <!-- Social -->
    <meta property="og:url" content="/" />
    <meta property="og:title" content="Allen Wang" />
    <meta
      property="og:description"
      content="Hi there, I'm Allen! I'm building intelligent systems to power a more sustainable world at Nectar."
    />

    <meta property="og:type" content="website" />

    <!-- Favicon -->
    <link rel="icon" href="favicon.ico" />
    <link rel="apple-touch-icon" href="apple-touch-icon.png" />
    <link rel="manifest" href="manifest.webmanifest" />

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-9RPG21TB1F"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-9RPG21TB1F");
    </script>

    <!-- Style -->
    <link rel="stylesheet" href="index.css" />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"
    />
  </head>

  <body>
    <div id="container">
      <div id="header">
        <h1>Allen Wang</h1>
        <!-- <div class="tab">writing</div> -->
      </div>
      <div id="headshot">
        <img id="headshot" src="headshot.jpg" />
      </div>
    </div>
    <div id="intro">
      <p>
        Hi there, I'm Allen! I'm building intelligent systems to power a
        more sustainable world at
        <a href="https://nectarclimate.com" target="blank">Nectar</a>. We apply AI advancements to disrupt the way companies buy energy for their operations.
      </p>
      <p>
        I grew up in a quiet town in Massachusetts, where I studied math and chemistry. At MIT, I discovered a passion for building software. I owe the positive life experiences over the years to serendipitous friendships and whimsical side-quests. I'm grateful to have the rare opportunity to work on hard problems with super talented people.
      </p>
    </div>

    <div class="section">
      <h2>Nectar</h2>
      <p>
        <a href="https://nectarclimate.com" target="blank">Nectar</a> is an AI climate-tech company that's building the future of AI infrastructure sustainably. We believe that for AI to reach massive adoption across all industries globally, the following must be accomplished:
      </p>
        <ol>
            <li>
                Easy access to cheap and clean energy to power datacenters and <a href="https://www.activesustainability.com/climate-change/100-companies-responsible-71-ghg-emissions/" target="blank">emissive companies</a>
            </li>
            <li>
                Agent-friendly web infrastructure that allows web-agents to iterate at scale
            </li>
            <li>
                Feature extraction from unstructured pdfs, images, and html with 99.999% accuracy
            </li>
        </ol>
        <p>
            At Nectar, we're building (1) externally and (2) + (3) internally. Our main product monitors companies' energy usage and procures electricity at the best price for their needs. To solve this problem at scale, we built scrapers to download terrabytes of data and data pipelines to pull that into one organized schema. We aspire to one day open source our internal products to help the community build more sustainable AI.
        </p>
        <p>
            Example of some things that we've built: 
            <ul>
                <li>
                    <strong>Text forwarding service on a raspberry pi</strong> to allow access to MFA / 2FA protected websites. We tried Google Voice, Twilio, and a few other solutions; and we realized that to do it right, we had to do it ourselves.
                </li>
                <li>
                    <strong>Self-hosted infrastructure to scrape websites in headed mode</strong>, building on top of <a href="https://www.browserbase.com/" target="blank">Browserbase</a> and its competitors. Our scrapers run from Mac minis in our office proxied through residential IP addresses.
                </li>
                <li>
                    <strong>99% field-level accuracy on structured data extraction</strong> from unstructured documents, images, and html. We use <a href="https://temporal.io" target="blank">temporal</a> for orchestration and workflow management and leverage <a href="https://reducto.ai/" target="blank">Reducto</a> for OCR.
                </li>
            </ul>
        </p>
        <p>
            <strong>However, we still have a long way to go.</strong> Most notably, 99% isn't good enough for our product: each document needs around 1000 JSON fields to be extracted, so to acheive 99% perfection at a document level, we require 99.999% field-level accuracy (assuming a uniform distribution of errors). Unfortunately, existing AI tools often prioritize latency over accuracy because highly valued AI companies like coding or customer service agents can tolerate occasional hallucinations and need immediate responses. Our product requires more 9's in the SLA and we can afford to wait on more LLM calls. So, most existing frameworks don't work for us, and we end up building a lot in-house. Some of the problems we're working on right now:
        <ul>
            <li>
                <strong>Prompt design abstractions:</strong> We spend some time thinking about how to make prompt design more modular, prioritizing reusability and testability. Sometimes slightly different tasks only require small changes to the prompt, and we'd like to keep things <a href="https://en.wikipedia.org/wiki/Don%27t_repeat_yourself" target="blank">DRY</a>. Frameworks like <a href="https://www.langchain.com/" target="blank">Langchain</a> end up abstracting away too much of the design process, making debugging and testing more difficult. Cursor's <a href="https://github.com/anysphere/priompt" target="blank">priompt</a> is a strong move in the right direction. However, our use case differs from Cursor's priompt because we're much more accuracy-sensitive. When the context window is limiting, priompt truncates the input to fit into the 200k context window. Truncation risks losing critical context, so we have to either divide and conquer or compress the context losslessly. How can we build iterate on priompt for 99.999% SLA applications?
            </li>
            <li>
                <strong>Long list extraction:</strong> Imagine trying to extract a list of unique people mentioned in a 200-paged magazine. When the extracted list is long, we face challenges in LLMs becoming lazy or duplicating data it's already seen. There are tricky logical duplicates that we can't easily handle with simple deduplication (e.g. "Barack Obama's daughter's mother", and "the first Lady", and "Mrs. Obama" are considered the same person, but if Mrs. Obama actually means Barack Obama's mother, then it's considered different). Even worse, when we divide list extraction into multiple steps, often we lose the thoughts / reasoning in the earlier steps and struggle to retain that context. What tools do we need to give LLMs to help with list extraction and deduplication? Will a scratchpad for writing thoughts suffice?
            </li>
            <li>
                <strong>Scalable scraper infrastructure:</strong> We have a large number of scrapers that we use to download data from the internet. We need to make sure that we can scale these scrapers to terabytes of data per day, avoid rate limiting, solve recaptcha, and more. <a href="https://www.reworkd.ai/" target="blank">Reworkd</a>, <a href="https://www.browserbase.com/" target="blank">Browserbase</a>, and <a href="https://brightdata.com/" target="blank">Brightdata</a> have made good progress, but we still need to build a lot of infrastructure to make sure that we can scale to our needs.
            </li>
            <li>
                <strong>Schema design:</strong> One unique challenge of our product is that we track relationships between objects that are time-dependent. E.g. a building in our database may be associated with customer number 123 in 2024 but can become associated with customer 456 in 2025. How can we track and query these relationships with time as new dimension? For utility companies in particular, sometimes there are multiple ids (your account number vs your meter number, etc.) that are associated with an account, and we need to build in the flexibility to allow our customers to query by their id of choice. How can we build a schema that scales and is flexible enough to serve our customers' needs while trying to follow principles like <a href="https://en.wikipedia.org/wiki/Third_normal_form" target="blank">3NF</a>?
            </li>
            <li>
                <strong>Testing infrastructure:</strong> Most ML tools like <a href="https://wandb.ai/site/" target="blank">Weights and Biases</a> are designed for model testing when training costs dominate the testing costs. For AI applications building, testing cost ends up dominating (costs between $0.20 - $2.00 per document). Caching helps a little, but we still can't run our data pipeline on 1000s of documents for each PR just to test a new approach. At the same time, we need to make sure that when we improve the model for one type of document, performance doesn't regress on other documents. One interesting proposal was to build a large test set of documents and tag which functions / prompts in the codebase are responsible for the performance of the pipeline on the document. Then on each push, we sample from the list of documents that would be affected. Obviously, this still feels quite expensive to maintain, but it's a start. What tools should we build for efficient testing?
            </li>
        </ul>
        </p>
        <p>
            <strong>If you have interesting ideas on these problems, or think you'd have a blast working on them, you should consider joining our team full-time.</strong> We're a talent-dense group of researchers, engineers, and hackers based in SF. If you'd like to chat, please reach out to me at allen [at] nectarclimate [dot] com.
        </p>
    </div>

    <div class="section">
      <h2>Previously</h2>
      <ul>
        <li>Helped close <a href="https://scale.com/" target="blank">Scale AI</a>'s first customer xM+ ARR for catalog scraping in 2021</li>
        <li>Built <a href="https://dialogic.live" target="blank">Dialogic</a>, a platform used by 1000+ teachers today for tracking student discussions and contributions</li>
        <li>Published <a href="https://www.sciencedirect.com/science/article/pii/S019566981930160X" target="blank">Relationship between Mullineux involution and the generalized regularization</a> in <i>European Journal of Combinatorics</i></li>
        <li>Designed <a href="https://cryptogram.allenwang314.com/report" target="blank">cipher-decryption methods</a> and benchmarked <a href="https://github.com/6851-2021/matrix-walker" target="blank">cache oblivious indexing strategies</a></li>
        <li>
          Led the dev team at
          <a href="https://hackmit.org" target="blank">HackMIT</a> and helped build the initial
          versions their judging and registration systems
        </li>

        <li>Organized a <a href="https://abmathcompetitions.org" target="blank">math competition</a> and participated in USAMO, USNCO, USAMTS, etc.; and later, built <a href="https://autocomp.vercel.app/" target="blank">Autocomp</a> to host online contests</li>
      </ul>
    </div>

    <div class="section">
      <h2>Perspectives</h2>
      <p>
        I'm generally interested in energy, security, and AI. There are days where I'm super techno-optimistic and other days where I feel like <a href="https://www.youtube.com/watch?v=eWynt87PaJ0&t=345s" target="blank">we have much more work to do</a>. Some of my core ideas are below.
      </p>
      <ul>
        <li>
          <strong>Energy: We do not have enough energy on Earth to sustain our current lifestyles or our AI-powered future.</strong> More AI means more energy demand. Better solar cells and better incentives means clean energy is (for the most part) cheaper. Why are companies not buying cleaner and cheaper energy? They need tools to help them find the best deals. This is the problem we're solving at <a href="https://nectarclimate.com" target="blank">Nectar</a>.
        </li>
        <li>
            <strong>Security: The future of defense will be in system security.</strong> <a href="https://en.wikipedia.org/wiki/Zero-day_(computing)" target="blank">Zero-day exploits</a> will become easier to find. IDF uses <a href="https://en.wikipedia.org/wiki/Pegasus_(spyware)" target="blank">Pegasus</a> to read your iMessages; the CIA composed <a href="https://en.wikipedia.org/wiki/Stuxnet" target="blank">four zero-days</a> to sabotage Iran's nuclear weapons program under Barack Obama; and recently, China hacked the US <a href="https://www.cbsnews.com/news/chinese-hackers-us-department-of-treasury/" target="blank">department of Treasury</a>. AI empowers hackers and foreign enemies to discover vulnerabilities more cheaply and scalably. With AI coding, more vulnerable code is being written faster than ever. Stronger security frameworks are required to protect our national security and protect our civil liberties from domestic intelligence agencies. 
        </li>
        <li>
            <strong>AI: <a href="https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7" target="blank">We are still in the early days of AI</a>.</strong> We mastered the theory of "small models" in classical ML and burned trillions of dollars in large models' experiments. However, we have hand-wavy explanations at best for the inner workings of large models and some intuitive understanding of the <a href="http://misha.belkin-wang.org/" target="blank">double descent curve</a>. The amazing LLMs we see today lack the theoretical underpinnings to explain their behavior, but that's how humanity has always worked—we <a href="https://hdsr.mitpress.mit.edu/pub/wot7mkc1/release/10" target="blank">built bridges long before civil engineering was a field</a>. Even after the industrial revolution, we needed centuries of physics from Einstein, Farraday, Fermi, Oppenheimer, Bohr, Heisenberg, etc. to develop the foundational knowledge to build skyscrapers and landing on the moon. <strong>The LLMs of today are the beautiful bridges that have taken us already far. It is difficult to imagine what the skyscrapers of ML will be. My intuition is that we'll need breakthroughs in <a href="https://en.wikipedia.org/wiki/Information_geometry" target="blank">information geometry</a> to get there.</strong>
        </li>
      </ul>
    </div>

    <div class="footer">
        <a
        href="https://github.com/AllenWang314"
        target="blank"
        class="fa fa-2x fa-github"
        style="text-decoration: none;"
        ></a>
        <a
        href="https://www.linkedin.com/in/allenwang314/"
        target="blank"
        class="fa fa-2x fa-linkedin"
        style="text-decoration: none;"
        ></a>
        (AllenWang314 mostly, Allen Wang © 2025)
    </div>
  </body>

  <script src="index.js"></script>
</html>
